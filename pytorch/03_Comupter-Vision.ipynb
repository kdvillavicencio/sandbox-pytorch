{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Computer Vision & CNN\n",
    "#coding/python #coding/deeplearning\n",
    "\n",
    "**References**\n",
    "- [YT: Learn PyTorch for Deep Learing](https://youtu.be/Z_ikDlimN6A?t=50419)\n",
    "- [LearnPyTorch.io](https://www.learnpytorch.io/03_pytorch_computer_vision/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Introduction to computer vision\n",
    "**Potential Applications**\n",
    "- BCC - dog or cat?\n",
    "- MCC - what kind of food?\n",
    "- object detection\n",
    "- segmentation\n",
    "\n",
    "**Outline**\n",
    "- getting vision dataset using `torchvision.datasets`\n",
    "- architecture of CNN\n",
    "- E2E multi-class image classification\n",
    "- steps in modeling with CNN\n",
    "  - create CNN model\n",
    "  - pick loss & optimizer\n",
    "  - train a model\n",
    "  - evaluate a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Computer vision input and outputs\n",
    "- CNN usually used in image data\n",
    "\n",
    "**Method**\n",
    "- numerically represent images as tensors\n",
    "  - (NHWC): color channels last\n",
    "  - (NCHW): color channels first\n",
    "- feed into ML algorithm\n",
    "- probabilites\n",
    "\n",
    "**Steps**\n",
    "1. Data Preperation\n",
    "  - `torchvision.transforms`\n",
    "  - `torch.utils.data.Dataset`\n",
    "  - `torch.utils.data.DataLoader`\n",
    "2. Build Model\n",
    "  - `torch.nn`\n",
    "  - `torch.nn.Module`\n",
    "  - `torchvision.models`\n",
    "3. Loss Fn & Optimizer\n",
    "  - `torch.optim`\n",
    "4. Evaluate Model\n",
    "  - `torchmetrics`\n",
    "5. Improve via Experimentation\n",
    "  - `torch.utils.tensorboard`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. What is a convolutional neural network?\n",
    "**Architecture**\n",
    "- input image\n",
    "  - target image\n",
    "- input layer\n",
    "  - processed target image\n",
    "  - `input_shape = [batch_size, ht, wd, color_channels]`\n",
    "- convolution layer\n",
    "  - extracts/learns features from target image\n",
    "  - `torch.nn.ConvXd()`\n",
    "- hidden activation/non-linear activation\n",
    "  - adds non-linearity\n",
    "  - usually `torch.nn.ReLU()`\n",
    "- pooling layer\n",
    "  - reduces dimensionality of learned features\n",
    "  - usually `torch.nn.MaxPool2d()`\n",
    "- output layer/linear layer\n",
    "  - takes learned features and outputs into shape of target label\n",
    "  - `torch.nn.Linear(out_features=[output-shape])\n",
    "- output activation\n",
    "  - convert output logits into prediction probabilities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. TorchVision\n",
    "**Computer Vision libraries**\n",
    "- `torchvision`\n",
    "- `torchvision.datasets`\n",
    "- `torchvision.models`\n",
    "- `torchvision.transforms`\n",
    "- `torch.utils.data.Dataset`\n",
    "- `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Getting a computer vision dataset\n",
    "Dataset: FashionMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup Training Data\n",
    "train_data = datasets.FashionMNIST\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=ToTensor(), # Transforms image or ndarray to tensor\n",
    "    target_transform=None,\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data), len(test_data)\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "class_names = train_data.classes\n",
    "print(f\"len(train_data), len(test_data)\")\n",
    "print(f\"{class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ankle boot')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize data\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "\n",
    "# Plot more images\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "rows, cols = 4,4\n",
    "for i in range(1, rows*cols+1):\n",
    "    random_idx = torch.randint(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Mini-batches\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Creating DataLoaders\n",
    "- Currently, data is in the form of PyTorch Datasets\n",
    "- DataLoader: turns dataset into a Python iterable\n",
    "\n",
    "**Creating Batches**\n",
    "1. allows the cpu to store smaller batches (vs full)\n",
    "   - computationally efficient\n",
    "   - common batch size = 32\n",
    "2. gives NN more chances to update its gradient per epoch\n",
    "\n",
    "**Steps**\n",
    "1. `torchvision.datasets.FashionMNIST`\n",
    "2. `torch.utils.data.DataLoader`\n",
    "   - `DataLoader(dataset, batch_size, shuffle=True)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1875 | 313\n",
      "torch.Size([32, 1, 28, 28]), torch.Size([32])\n",
      "Image size: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True)\n",
    "\n",
    "# Visualize\n",
    "print(f\"Length: {len(train_dataloader)} | {len(test_dataloader)}\")\n",
    "\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(f\"{train_features_batch.shape}, {train_labels_batch.shape}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a **baseline** model is a best practice, starting with a simple model and add complexity from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten()\n",
    "\n",
    "x = train_features_batch[0]\n",
    "print(f\"{x.shape}\")\n",
    "\n",
    "output = flatten_model(x)\n",
    "print(f\"{output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base model\n",
    "from torch import nn\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape:int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create base model instance\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape=28*28,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ")\n",
    "\n",
    "dummy_x = torch.rand([1,1,28,28])\n",
    "model_0(dummy_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n"
     ]
    }
   ],
   "source": [
    "# Import external file\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if Path('helper_functions.py').is_file():\n",
    "  print('File exists.')\n",
    "else:\n",
    "  print('Downloading file...')\n",
    "  request = requests.get('https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py')\n",
    "  with open('helper_functions.py', 'wb') as f:\n",
    "    f.write(request.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typical Evaluation Metrics**\n",
    "- model performance (loss and accuracy)\n",
    "- how fast it runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss, optimizer, and eval metrics\n",
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn_v0 = nn.CrossEntropyLoss()\n",
    "optimizer_v0 = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 0.000 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.308100763708353e-05"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup timer metric\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float, end:float, device:torch.device=None):\n",
    "  total_time = end - start\n",
    "  print(f\"Train time: {total_time:.3f} seconds\")\n",
    "  return total_time\n",
    "\n",
    "start_time = timer()\n",
    "end_time = timer()\n",
    "print_train_time(start=start_time, end=end_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. Training and testing loops for batched data\n",
    "Creating a training loop\n",
    "- loop through epochs\n",
    "- loop through training batches\n",
    "- loop through testing batches\n",
    "- print out what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Looked at 0/60000 samples.\n",
      "Looked at 12800/60000 samples.\n",
      "Looked at 25600/60000 samples.\n",
      "Looked at 38400/60000 samples.\n",
      "Looked at 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:15,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.5904 | Test loss: 0.5102, test acc: 82.0387\n",
      "Epoch: 1\n",
      "Looked at 0/60000 samples.\n",
      "Looked at 12800/60000 samples.\n",
      "Looked at 25600/60000 samples.\n",
      "Looked at 38400/60000 samples.\n",
      "Looked at 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:15<00:07,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.4743 | Test loss: 0.5268, test acc: 81.6793\n",
      "Epoch: 2\n",
      "Looked at 0/60000 samples.\n",
      "Looked at 12800/60000 samples.\n",
      "Looked at 25600/60000 samples.\n",
      "Looked at 38400/60000 samples.\n",
      "Looked at 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:22<00:00,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.4537 | Test loss: 0.5126, test acc: 82.9972\n",
      "Train time: 22.502 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42)\n",
    "train_time_start = timer()\n",
    "\n",
    "epochs = 3 \n",
    "\n",
    "# Create train & test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f\"Epoch: {epoch}\")\n",
    "\n",
    "  ### TRAINING\n",
    "  train_loss = 0\n",
    "  ## NOTE: model parameters are updated per batch\n",
    "  for batch, (X, y) in enumerate(train_dataloader):\n",
    "    model_0.train()\n",
    "    # 1. Forward pass\n",
    "    y_pred = model_0(X)\n",
    "    # 2. Calculate the loss\n",
    "    loss = loss_fn_v0(y_pred, y)\n",
    "    train_loss += loss # to be averaged per batch\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer_v0.zero_grad()\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "    # 5. Optimizer step\n",
    "    optimizer_v0.step()\n",
    "\n",
    "    if batch % 400 == 0:\n",
    "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\")\n",
    "\n",
    "  train_loss /= len(train_dataloader)\n",
    "\n",
    "  ### TESTING\n",
    "  test_loss, test_acc = 0, 0\n",
    "  model_0.eval()\n",
    "  with torch.inference_mode():\n",
    "    for X_test, y_test in test_dataloader:\n",
    "      # 1. Forward pass\n",
    "      test_pred = model_0(X_test)\n",
    "      # 2. Calc loss\n",
    "      test_loss += loss_fn_v0(test_pred, y_test)\n",
    "      # 3. Calc accuracy\n",
    "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "  print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, test acc: {test_acc:.4f}\")\n",
    "\n",
    "train_time_end = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start, end=train_time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.5119837522506714,\n",
       " 'model_acc': 83.02715654952077}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn):\n",
    "  loss, acc = 0, 0\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for X, y in data_loader:\n",
    "        # Make predictions\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss += loss_fn(y_pred, y)\n",
    "        acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "    \n",
    "    loss /= len(data_loader)\n",
    "    acc /= len(data_loader)\n",
    "\n",
    "  return {\"model_name\": model.__class__.__name__,\n",
    "          \"model_loss\": loss.item(),\n",
    "          \"model_acc\": acc,\n",
    "          }\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model_0, data_loader=test_dataloader, loss_fn=loss_fn_v0, accuracy_fn=accuracy_fn)\n",
    "model_0_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Running experiments on the GPU\n",
    "https://youtu.be/Z_ikDlimN6A?t=59160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating a model with non-linear functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Creating a train/test loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Convolutional neural networks (overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Coding a CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Breaking down nn.Conv2d/nn.MaxPool2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Training our first CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Making predictions on random test samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Plotting our best model predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Evaluating model predictions with a confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
